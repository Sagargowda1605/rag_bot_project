{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56600cc0-2de4-4949-892a-e86b18c053f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings  \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e011bcc-b530-4b5f-a804-26239d7b09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are using text file here, textloader libararie is used to read the file \n",
    "loader=TextLoader(\n",
    "    file_path=\"internal_manual.txt\")\n",
    "#we are loading the the data into the lanhchain document using load function what it does is takes the lanhchain object andguives the text in the list form \n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55744e6a-779a-4cb3-b510-4f53b1942d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the can be big and llm have limitataions how much they can read we have splite the entire data into chunks and store them \n",
    "#so here we are using the recrusive text splitter which helps maiantis the semantic meanings by allowing the over lapping \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f33cc5-9e98-4384-a015-a5241c967eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next thiung is we have to create the embeddings for the text for that we need an transfomer model and since we are running an locally everything we are using an HuggingFaceEmbeddings which is freeensures zero API cost and high portability.\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ce8053-940c-4cc0-aa21-5a7d39fdedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the we need the database to store vector for that we are using an chroma and chromadb chroma takes the texts and emedding model and conevrts them into vectors and stores in the vector databse chromadb \n",
    "from langchain_chroma import Chroma\n",
    "db = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=hf,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38475ca-ce53-4fdd-a08a-2f2855723d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#ok now we have done the text loading, splitting,and embeddings and piplien to use convert the text into embeddings and vector database, now we need an llm to generate the response of our question \n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "hf_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-small\",\n",
    "    task=\"text2text-generation\", # The correct, flexible task\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526fb908-1407-4b57-bf22-bc66f0be5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are selecting the search type to be mmr because to avoid the reduduncy maximal marginal redudnace reduce sleceting the same info type documents and makes sures that new document that is selected contains new information \n",
    "#Balances relevance and diversity: MMR calculates a score that considers both how similar a document is to the original query and how different it is from documents already chosen. \n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 10}, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a62d6bd-e14b-4342-ace7-82d3056d983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_llm(llm=hf_llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5605e2f5-1a96-4caa-b1d6-bdedf34e4ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'tell me about the warranty duration',\n",
       " 'result': 'thirty-six (36) months'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"query\": \"tell me about the warranty duration\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786b187f-9c78-4811-8f5a-adb40571751d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain me about the Limited Product Warranty Policy, breifly exaplin ',\n",
       " 'result': 'The warranty specifically excludes any claims arising from user-induced modifications, data'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"query\": \"Explain me about the Limited Product Warranty Policy, breifly exaplin \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972070a-140f-423b-ab0b-9f6d338741d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
